{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QuantBook Analysis Tool \n",
    "# For more information see [https://www.quantconnect.com/docs/v2/our-platform/research/getting-started]\n",
    "qb = QuantBook()\n",
    "spy = qb.AddEquity(\"SPY\")\n",
    "history = qb.History(qb.Securities.Keys, 360, Resolution.Daily)\n",
    "\n",
    "# Indicator Analysis\n",
    "bbdf = qb.Indicator(BollingerBands(30, 2), spy.Symbol, 360, Resolution.Daily)\n",
    "bbdf.drop('standarddeviation', axis=1).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qb = QuantBook()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def efficient_frointier_on_sharp(cov_mat,exp_vec,rf):\n",
    "    \"\"\"\n",
    "    Output: weights; sharp ratio\n",
    "    \"\"\"\n",
    "    f = lambda x: -1*(exp_vec@x - rf)/np.sqrt(x@cov_mat@x.T)\n",
    "    n = len(exp_vec)\n",
    "    # boundary = ((0, 1) for _ in range(n))\n",
    "    constraints = {'type':'eq', 'fun': lambda x: np.sum(x) - 1}\n",
    "    opt = optimize.minimize(f,np.ones(n)/n,constraints=constraints)\n",
    "    return opt.x,-opt.fun\n",
    "\n",
    "## risk parity portfolio construction\n",
    "def port_vol_calculate(weights,covariance):\n",
    "    variance = weights@covariance@weights.T\n",
    "    return np.sqrt(variance)\n",
    "\n",
    "def component_std_calculate(weights,covariance):\n",
    "    port_vol = port_vol_calculate(weights,covariance)\n",
    "    return weights*(covariance@weights.T)/port_vol\n",
    "\n",
    "def component_std_sse_calculate(weights,covariance,budget=None):\n",
    "    if not budget:\n",
    "        budget = np.ones_like(weights)\n",
    "    csd = component_std_calculate(weights,covariance)/budget\n",
    "    scale_csd = csd - csd.mean()\n",
    "    sse = scale_csd @ scale_csd.T\n",
    "\n",
    "    return sse\n",
    "\n",
    "def risk_parity_portfolio_on_sse(covariance,budget = None):\n",
    "    n = covariance.shape[0]\n",
    "    cons = {'type':'eq',\"fun\":lambda w:np.sum(w)-1}\n",
    "    bounds = ((0,1) for i in range(n))\n",
    "    opt_result = optimize.minimize(lambda w: 1e3*component_std_sse_calculate(w,covariance,budget),x0 = np.array([1/n]*n),constraints=cons,bounds=bounds)\n",
    "\n",
    "    return opt_result.x\n",
    "\n",
    "## construct the risk portfolio based on the es\n",
    "def component_es(weights,returns,delta = 1e-6):\n",
    "    n = len(weights)\n",
    "    port_es = Return_ES(returns@weights.T)\n",
    "    es_list = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        ind_w = weights[i]\n",
    "        weights[i] += delta\n",
    "        es_list[i] = ind_w * (Return_ES(returns@weights.T)-port_es)/delta\n",
    "        weights[i] = ind_w\n",
    "\n",
    "    return es_list\n",
    "\n",
    "def component_es_sse(weights,returns,budget,delta = 1e-6):\n",
    "    \"\"\"\n",
    "    Budge should be within the list form.\n",
    "    \"\"\"\n",
    "    if not budget:\n",
    "        budget = np.ones_like(weights)\n",
    "    ces= component_es(weights,returns,delta)/budget\n",
    "    scale_com_es = ces - ces.mean()\n",
    "    return scale_com_es @ scale_com_es.T\n",
    "\n",
    "def risk_parity_port_es(returns,budget=None):\n",
    "    n = returns.shape[1]\n",
    "    cons = {'type':'eq','fun':lambda w:np.sum(w) - 1}\n",
    "    bounds = ((0,1) for i in range(n))\n",
    "    opt_result = optimize.minimize(lambda w: 1e5*component_es_sse(w,returns,budget),x0 = np.ones(n)/n,constraints=cons,bounds=bounds)\n",
    "\n",
    "    return opt_result.x\n",
    "\n",
    "def Return_ES(sim_x,alpha=0.05):\n",
    "    order_x = np.sort(sim_x)\n",
    "    n = alpha*len(order_x)\n",
    "    up_n = int(np.ceil(n))\n",
    "    dn_n = int(np.floor(n))\n",
    "    VaR = (order_x[up_n+1]+order_x[dn_n-1])/2\n",
    "    ES = -1*np.mean(order_x[order_x <= VaR])\n",
    "    return ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To extract portfolio NAV from backtest results \n",
    "backtest1 = api.ReadBacktest(16527795, 'd8440577cc94ba7f8f4dc3a4c6523a8d') \n",
    "\n",
    "backtest2 = api.ReadBacktest(16527795, '1956e15c56f7188cde0a6faf081cdd5b') \n",
    "chartpoint1_ls = backtest1.Charts[\"Strategy Equity\"].Series[\"Equity\"].Values \n",
    "chartpoint2_ls = backtest2.Charts[\"Strategy Equity\"].Series[\"Equity\"].Values \n",
    "nav1 = [x.Close for x in chartpoint1_ls if x.Time.hour != 5] \n",
    "nav2 = [x.Close for x in chartpoint2_ls if x.Time.hour != 5] \n",
    "date = [x.Time for x in chartpoint1_ls if x.Time.hour != 5] \n",
    "nav1 = pd.Series(nav1,index=date)\n",
    "nav2 = pd.Series(nav2,index=date)\n",
    "re_strategy1 = nav1.pct_change()\n",
    "re_strategy2 = nav2.pct_change()\n",
    "## effcient frontier optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "backtest_2023 = api.ReadBacktest(16527795, 'cff5189b6f03630c98a2e420f5eea720') \n",
    "cp_2023 = backtest_2023.Charts[\"Strategy Equity\"].Series[\"Equity\"].Values \n",
    "nav_23 = [x.Close for x in cp_2023 if x.Time.hour != 5] \n",
    "date_23 = [x.Time for x in cp_2023 if x.Time.hour != 5] \n",
    "nav_23 = pd.Series(nav_23,index=date_23)*10\n",
    "nav_23 = nav_23.loc[(nav_23.index<='2023-10-10')&(nav_23.index>='2023-3-10')]\n",
    "rtn_23 = nav_23.pct_change()\n",
    "plt.figure(figsize=(20,10),dpi=100, edgecolor='grey')\n",
    "plt.plot(nav_23, linewidth=3, color='orange')\n",
    "plt.title(\"\")\n",
    "plt.grid(\n",
    "         linestyle='--',\n",
    "         linewidth=1,\n",
    "         alpha=0.3) \n",
    "plt.savefig('2023_pv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.power(np.prod(1+rtn_23),1/len(nav_23))-1)/np.std(rtn_23) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(rtn_23)/np.std(rtn_23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.power(np.prod(1+rtn_23),252/len(nav_23))-1) - np(1+0.025) /(np.sqrt(252/len(nav_23))*np.std(rtn_23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(nav_23.apply(np.log).diff().dropna())*252/(np.sqrt(252/len(nav_23))*np.std(rtn_23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.power(np.prod(1+rtn_23),1/len(nav_23))-1)/np.std(rtn_23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(nav_23/nav_23.rolling(10).max() -1).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(re_strategy1)\n",
    "plt.plot(re_strategy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtn_df = pd.concat((re_strategy1, re_strategy2),axis=1).dropna()\n",
    "cov = rtn_df.cov()\n",
    "rtn_df.mean().values\n",
    "efficient_frointier_on_sharp(exp_vec=rtn_df.mean().values,cov_mat=cov.values, rf=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_parity_port_es(rtn_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from A2C import A2C_Trading, StockTradingEnv\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import json\n",
    "\n",
    "import joblib\n",
    "def train_epoch(model, env,features, price, optimizer, epoch, gamma=0.99,std_dev =torch.tensor([0.05])):\n",
    "    state = env.reset()\n",
    "    \n",
    "    episode_reward = 0\n",
    "    # Initialize LSTM hidden and cell states\n",
    "    hidden_critic = None\n",
    "    hidden_actor = None\n",
    "    count = 1\n",
    "    alpha = 0.95\n",
    "    epsilon = 0.01\n",
    "    ema_reward = 0\n",
    "    port_vals = []\n",
    "    balances = []\n",
    "    ports = []\n",
    "    actions = []\n",
    "    current_step = lookback_window_size\n",
    "    while True:\n",
    "        # Convert state to appropriate tensor format for the model\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        window_start = current_step - env.lookback_window_size\n",
    "        window_end = current_step\n",
    "        current_step +=10\n",
    "        this_feature = features[:,:,window_start:window_end]\n",
    "        this_price = price[window_end,:]\n",
    "        count += 1\n",
    "        # Get action probabilities and state value from the model\n",
    "\n",
    "        \n",
    "        action_mean, state_value, hidden_actor, hidden_critic = model(state_tensor, hidden_actor, hidden_critic)\n",
    "        \n",
    "        hidden_actor = (hidden_actor[0].detach(), hidden_actor[1].detach())\n",
    "        hidden_critic = (hidden_critic[0].detach(), hidden_critic[1].detach())\n",
    "        \n",
    "        # Sample action from the probability distribution\n",
    "        action = torch.normal(action_mean, std_dev)  \n",
    "        action = action.clamp(-0.2, 0.2)  # Clamp to action space\n",
    "\n",
    "        # Take action in the environment\n",
    "        #print(action)\n",
    "        if torch.isnan(action).any():\n",
    "            \n",
    "            action_np =np.zeros(action.detach().numpy().shape)\n",
    "        else:\n",
    "            action_np = action.detach().numpy()\n",
    "            \n",
    "        next_state, reward, done = env.step(action_np,this_feature, this_price)\n",
    "        ema_reward = ema_reward * alpha + reward * (1 - alpha)\n",
    "        if reward < 0:\n",
    "            reward *= 2\n",
    "        normalized_reward = reward / (ema_reward + epsilon)\n",
    "        episode_reward += reward\n",
    "\n",
    "        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "\n",
    "        _, next_state_value, _,_ = model(next_state_tensor, hidden_actor, hidden_critic)\n",
    "    \n",
    "        # Calculate advantage\n",
    "        advantage = normalized_reward + (gamma * next_state_value * (1 - int(done))) - state_value.detach()\n",
    "\n",
    "        # Calculate the log probabilities\n",
    "        variances = std_dev ** 2\n",
    "        log_probs = -((action - action_mean) ** 2) / (2 * variances) - np.log(std_dev * np.sqrt(2 * torch.pi))\n",
    "\n",
    "        # Actor loss\n",
    "        actor_loss = -torch.mean(log_probs * advantage.detach())\n",
    "        \n",
    "        \n",
    "        # Critic loss\n",
    "        critic_loss = torch.nn.functional.mse_loss(state_value, normalized_reward + gamma * next_state_value.detach() * (1 - int(done)))\n",
    "\n",
    "        # Aggregate losses\n",
    "        total_loss = actor_loss + critic_loss\n",
    "\n",
    "        # Perform backpropagation on total loss\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Transition to the next state\n",
    "        state = next_state\n",
    "        port_vals.append(env.total_portfolio_value)\n",
    "        balances.append(env.balance)\n",
    "        actions.append(action_np)\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    env.render()\n",
    "    print(\"Epoch{}: rewards: {:.4f}\".format(epoch, env.total_portfolio_value-env.initial_balance))\n",
    "    return episode_reward, port_vals,balances,ports,actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, env,features, price, optimizer, num_epoch=50, gamma=0.99,std_dev =torch.tensor([0.03]), model_key=\"A2Cmodel\"):\n",
    "    rewards =[]\n",
    "    hidden_critic = None\n",
    "    hidden_actor = None\n",
    "    max_reward = 0\n",
    "    for i in range(1, num_epoch):\n",
    "        state = env.reset()\n",
    "    \n",
    "        episode_reward = 0\n",
    "        # Initialize LSTM hidden and cell states\n",
    "        \n",
    "        count = 1\n",
    "        alpha = 0.95\n",
    "        epsilon = 0.01\n",
    "        ema_reward = 0\n",
    "        port_vals = []\n",
    "        balances = []\n",
    "        ports = []\n",
    "        actions = []\n",
    "        current_step = lookback_window_size\n",
    "        while True:\n",
    "            # Convert state to appropriate tensor format for the model\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            window_start = current_step - env.lookback_window_size\n",
    "            window_end = current_step\n",
    "            current_step +=10\n",
    "            this_feature = features[:,:,window_start:window_end]\n",
    "            this_price = price[window_end,:]\n",
    "            count += 1\n",
    "            # Get action probabilities and state value from the model\n",
    "\n",
    "        \n",
    "            action_mean, state_value, hidden_actor, hidden_critic = model(state_tensor, hidden_actor, hidden_critic)\n",
    "        \n",
    "            hidden_actor = (hidden_actor[0].detach(), hidden_actor[1].detach())\n",
    "            hidden_critic = (hidden_critic[0].detach(), hidden_critic[1].detach())\n",
    "        \n",
    "            # Sample action from the probability distribution\n",
    "            action = torch.normal(action_mean, std_dev)  \n",
    "            action = action.clamp(-0.1, 0.1)  # Clamp to action space\n",
    "\n",
    "            # Take action in the environment\n",
    "            #print(action)\n",
    "            if torch.isnan(action).any():\n",
    "            \n",
    "                action_np =np.zeros(action.detach().numpy().shape)\n",
    "            else:\n",
    "                action_np = action.detach().numpy()\n",
    "            \n",
    "            next_state, reward, done = env.step(action_np,this_feature, this_price)\n",
    "            if reward<0:\n",
    "                reward *=2\n",
    "            ema_reward = ema_reward * alpha + reward * (1 - alpha)\n",
    "            normalized_reward = reward / (ema_reward + epsilon)\n",
    "            episode_reward += reward\n",
    "            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "\n",
    "            _, next_state_value, _,_ = model(next_state_tensor, hidden_actor, hidden_critic)\n",
    "    \n",
    "            # Calculate advantage\n",
    "            advantage = normalized_reward + (gamma * next_state_value * (1 - int(done))) - state_value.detach()\n",
    "\n",
    "            # Calculate the log probabilities\n",
    "            variances = std_dev ** 2\n",
    "            log_probs = -((action - action_mean) ** 2) / (2 * variances) - np.log(std_dev * np.sqrt(2 * torch.pi))\n",
    "\n",
    "            # Actor loss\n",
    "            actor_loss = -torch.mean(log_probs * advantage.detach())\n",
    "        \n",
    "        \n",
    "            # Critic loss\n",
    "            critic_loss = torch.nn.functional.mse_loss(state_value, normalized_reward + gamma * next_state_value.detach() * (1 - int(done)))\n",
    "\n",
    "            # Aggregate losses\n",
    "            total_loss = actor_loss + critic_loss\n",
    "\n",
    "            # Perform backpropagation on total loss\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "            # Transition to the next state\n",
    "            state = next_state\n",
    "            port_vals.append(env.total_portfolio_value)\n",
    "            balances.append(env.balance)\n",
    "            actions.append(action_np)\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        env.render()\n",
    "        rewards.append(episode_reward)\n",
    "        if env.total_portfolio_value-env.initial_balance > max_reward:\n",
    "            max_reward = env.total_portfolio_value-env.initial_balance\n",
    "            joblib.dump(model, file_name)\n",
    "        print(\"Epoch{}: rewards: {:.4f}\".format(i, env.total_portfolio_value-env.initial_balance))\n",
    "    \n",
    "    \n",
    "    return episode_reward, port_vals,balances,ports,actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AAPL R735QTJ8XC9X\n",
    "- GOOCV VP83T1ZUHROL\n",
    "- MSFT R735QTJ8XC9X\n",
    "- BRKB R735QTJ8XC9X\n",
    "- XON R735QTJ8XC9X\n",
    "- AMZN R735QTJ8XC9X\n",
    "- FB V6OIPNZEM8V9\n",
    "- JNJ R735QTJ8XC9X\n",
    "- CMB R735QTJ8XC9X\n",
    "- GE R735QTJ8XC9X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qb = QuantBook()\n",
    "\n",
    "jpm = qb.AddEquity(\"JPM\", Resolution.Daily).Symbol\n",
    "brk =  qb.AddEquity(\"BRK.B\", Resolution.Daily).Symbol\n",
    "aapl = qb.AddEquity(\"AAPL\", Resolution.Daily).Symbol\n",
    "amzn = qb.AddEquity(\"AMZN\", Resolution.Daily).Symbol\n",
    "msft = qb.AddEquity(\"MSFT\", Resolution.Daily).Symbol\n",
    "goog = qb.AddEquity(\"GOOG\", Resolution.Daily).Symbol\n",
    "xon = qb.AddEquity(\"XON R735QTJ8XC9X\", Resolution.Daily).Symbol\n",
    "fb = qb.AddEquity(\"FB V6OIPNZEM8V9\", Resolution.Daily).Symbol\n",
    "jnj = qb.AddEquity(\"JNJ\", Resolution.Daily).Symbol\n",
    "ge = qb.AddEquity(\"GE R735QTJ8XC9X\",Resolution.Daily).Symbol\n",
    "symbol_list = [jpm, brk,aapl, amzn, msft] #goog, xon, fb, jnj, ge\n",
    "start = datetime(2017, 1, 1)\n",
    "end = datetime(2022, 1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ema = ExponentialMovingAverage(30)\n",
    "rsi = RelativeStrengthIndex(30)\n",
    "arma = AutoRegressiveIntegratedMovingAverage(1,2,1,20, True)\n",
    "rdv = RelativeDailyVolume(10)\n",
    "bb = BollingerBands(20, 2)\n",
    "alp = Momentum(20)\n",
    "emas = {}\n",
    "rsis = {}\n",
    "armas = {}\n",
    "rdvs = {}\n",
    "bbs = {}\n",
    "alps = {}\n",
    "\n",
    "for symbol in symbol_list:\n",
    "    emas[symbol] = qb.Indicator(ema, symbol, start-timedelta(days=60),end, Resolution.Daily)\n",
    "    emas[symbol] = emas[symbol].loc[emas[symbol].index>=start]\n",
    "    rsis[symbol] = qb.Indicator(rsi, symbol, start-timedelta(days=60),end, Resolution.Daily)\n",
    "    rsis[symbol] = rsis[symbol].loc[rsis[symbol].index>=start]\n",
    "    armas[symbol] = qb.Indicator(arma, symbol, start-timedelta(days=60),end, Resolution.Daily)\n",
    "    armas[symbol] = armas[symbol].loc[armas[symbol].index>=start]\n",
    "    rdvs[symbol] = qb.Indicator(rdv, symbol, start-timedelta(days=60),end, Resolution.Daily)\n",
    "    rdvs[symbol] = rdvs[symbol].loc[rdvs[symbol].index>=start,:]\n",
    "    bbs[symbol] = qb.Indicator(bb, symbol, start-timedelta(days=60),end, Resolution.Daily)\n",
    "    bbs[symbol] = bbs[symbol].loc[bbs[symbol].index>=start]\n",
    "    alps[symbol] = qb.Indicator(alp, symbol, start-timedelta(days=60),end, Resolution.Daily)\n",
    "    alps[symbol] = alps[symbol].loc[alps[symbol].index>=start]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vols = {}\n",
    "for symbol in symbol_list:\n",
    "    vol = qb.History(symbol, start-timedelta(days=60),end, Resolution.Daily)['close'].unstack(level=1).T.rolling(30).std().dropna()\n",
    "    vol.columns = ['volatility']\n",
    "    vols[symbol] = vol\n",
    "    vols[symbol] = vols[symbol].loc[vols[symbol].index>=start,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators = [rsis,  armas, vols, rdvs, bbs,alps]\n",
    "price = qb.History(symbol_list, start,end, Resolution.Daily)['close'].unstack(level=1).T\n",
    "price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price.apply(np.log).diff().dropna().corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['relativestrengthindex','autoregressiveintegratedmovingaverage', 'volatility','relativedailyvolume',\t'bandwidth','percentb','momentum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for symbol in symbol_list:\n",
    "    df = None\n",
    "    for ind in indicators:\n",
    "        if df is None:\n",
    "            df = ind[symbol]\n",
    "        else:\n",
    "            df = df.join(ind[symbol])\n",
    "    features.append(df.loc[:,columns].values.T)\n",
    "\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,columns].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_stocks = len(symbol_list)  # example number of stocks\n",
    "num_features = features.shape[1]\n",
    "lookback_window_size = 30\n",
    "state_dim = lookback_window_size * num_stocks*num_features + 1 + num_stocks\n",
    "action_dim = num_stocks\n",
    "\n",
    "#A2C model\n",
    "model = A2C_Trading(state_dim, action_dim,hidden_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StockTradingEnv(features=features[:,:,:lookback_window_size],price=price.values[lookback_window_size,:],end_step=features.shape[2],lookback_window_size=lookback_window_size)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "model_key = \"A2Cmodel1\"\n",
    "file_name = qb.ObjectStore.GetFilePath(model_key)\n",
    "reward, port_vals,balances,ports,actions = train(model,env,features,price.values[:,:], optimizer, num_epoch=30,std_dev =torch.tensor([0.02]), model_key=\"A2Cmodel_c\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C_Trading(state_dim, action_dim,hidden_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StockTradingEnv(features=features[:,:,:lookback_window_size],price=price.values[lookback_window_size,:],end_step=features.shape[2],lookback_window_size=lookback_window_size)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "model_key = \"A2Cmodel2\"\n",
    "file_name = qb.ObjectStore.GetFilePath(model_key)\n",
    "\n",
    "max_reward = 10000\n",
    "num_epoch = 30\n",
    "rewards = []\n",
    "#torch.manual_seed(2550667350516364191) #3632672893473863851\n",
    "#torch.random.manual_seed(11249560557726264621) #35777981873920709\n",
    "for epoch in range(1, num_epoch + 1):\n",
    "    reward, port_vals,balances,ports,actions = train_epoch(model,env,features,price.values[:,:], optimizer, epoch)\n",
    "    rewards.append(reward)\n",
    "    if reward > max_reward:\n",
    "        max_reward = reward\n",
    "        joblib.dump(model, file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_key = \"A2Cmodel\"\n",
    "file_name = qb.ObjectStore.GetFilePath(model_key)\n",
    "joblib.dump(model, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_key = \"A2Cmodel\"\n",
    "file_name = qb.ObjectStore.GetFilePath(model_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load(file_name)\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StockTradingEnv(features=features[:,:,:lookback_window_size],price=price.values[lookback_window_size,:],end_step=2014,lookback_window_size=lookback_window_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "hidden_actor = None\n",
    "hidden_critic = None\n",
    "action_mean, state_value, hidden_actor, hidden_critic = loaded_model(state_tensor, hidden_actor, hidden_critic)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
