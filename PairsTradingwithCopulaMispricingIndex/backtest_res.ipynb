{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qb = QuantBook()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import optimize\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization Framework\n",
    "\n",
    "def efficient_frointier_on_sharp(cov_mat,exp_vec,rf):\n",
    "    \"\"\"\n",
    "    Output: weights; sharp ratio\n",
    "    \"\"\"\n",
    "    f = lambda x: -1*(exp_vec@x - rf)/np.sqrt(x@cov_mat@x.T)\n",
    "    n = len(exp_vec)\n",
    "    # boundary = ((0, 1) for _ in range(n))\n",
    "    constraints = {'type':'eq', 'fun': lambda x: np.sum(x) - 1}\n",
    "    opt = optimize.minimize(f,np.ones(n)/n,constraints=constraints)\n",
    "    return opt.x,-opt.fun\n",
    "\n",
    "## risk parity portfolio construction\n",
    "def port_vol_calculate(weights,covariance):\n",
    "    variance = weights@covariance@weights.T\n",
    "    return np.sqrt(variance)\n",
    "\n",
    "def component_std_calculate(weights,covariance):\n",
    "    port_vol = port_vol_calculate(weights,covariance)\n",
    "    return weights*(covariance@weights.T)/port_vol\n",
    "\n",
    "def component_std_sse_calculate(weights,covariance,budget=None):\n",
    "    if not budget:\n",
    "        budget = np.ones_like(weights)\n",
    "    csd = component_std_calculate(weights,covariance)/budget\n",
    "    scale_csd = csd - csd.mean()\n",
    "    sse = scale_csd @ scale_csd.T\n",
    "\n",
    "    return sse\n",
    "\n",
    "def risk_parity_portfolio_on_sse(covariance,budget = None):\n",
    "    n = covariance.shape[0]\n",
    "    cons = {'type':'eq',\"fun\":lambda w:np.sum(w)-1}\n",
    "    bounds = ((0,1) for i in range(n))\n",
    "    opt_result = optimize.minimize(lambda w: 1e3*component_std_sse_calculate(w,covariance,budget),x0 = np.array([1/n]*n),constraints=cons,bounds=bounds)\n",
    "\n",
    "    return opt_result.x\n",
    "\n",
    "## construct the risk portfolio based on the es\n",
    "def component_es(weights,returns,delta = 1e-6):\n",
    "    n = len(weights)\n",
    "    port_es = Return_ES(returns@weights.T)\n",
    "    es_list = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        ind_w = weights[i]\n",
    "        weights[i] += delta\n",
    "        es_list[i] = ind_w * (Return_ES(returns@weights.T)-port_es)/delta\n",
    "        weights[i] = ind_w\n",
    "\n",
    "    return es_list\n",
    "\n",
    "def component_es_sse(weights,returns,budget,delta = 1e-6):\n",
    "    \"\"\"\n",
    "    Budge should be within the list form.\n",
    "    \"\"\"\n",
    "    if not budget:\n",
    "        budget = np.ones_like(weights)\n",
    "    ces= component_es(weights,returns,delta)/budget\n",
    "    scale_com_es = ces - ces.mean()\n",
    "    return scale_com_es @ scale_com_es.T\n",
    "\n",
    "def risk_parity_port_es(returns,budget=None):\n",
    "    n = returns.shape[1]\n",
    "    cons = {'type':'eq','fun':lambda w:np.sum(w) - 1}\n",
    "    bounds = ((0,1) for i in range(n))\n",
    "    opt_result = optimize.minimize(lambda w: 1e5*component_es_sse(w,returns,budget),x0 = np.ones(n)/n,constraints=cons,bounds=bounds)\n",
    "\n",
    "    return opt_result.x\n",
    "\n",
    "def Return_ES(sim_x,alpha=0.05):\n",
    "    order_x = np.sort(sim_x)\n",
    "    n = alpha*len(order_x)\n",
    "    up_n = int(np.ceil(n))\n",
    "    dn_n = int(np.floor(n))\n",
    "    VaR = (order_x[up_n+1]+order_x[dn_n-1])/2\n",
    "    ES = -1*np.mean(order_x[order_x <= VaR])\n",
    "    return ES\n",
    "\n",
    "\n",
    "def calc_max_drawdown(pnl):\n",
    "    pnl_array = pnl.to_numpy().flatten()\n",
    "    drawdowns = []\n",
    "    max_drawdown_duration = 0\n",
    "    start = 0\n",
    "    max_so_far = pnl_array[0]\n",
    "    max_drawdown = 0\n",
    "    length = len(pnl_array)\n",
    "    for i in range(length):\n",
    "        if pnl_array[i] > max_so_far:\n",
    "            start = i\n",
    "            drawdown = 0\n",
    "            drawdowns.append(drawdown)\n",
    "            max_so_far = pnl_array[i]\n",
    "        else:\n",
    "            drawdown = pnl_array[i] - max_so_far\n",
    "            if drawdown <= max_drawdown:\n",
    "                max_drawdown = drawdown\n",
    "                max_drawdown_duration = max(max_drawdown_duration, i - start)\n",
    "            drawdowns.append(drawdown/max_so_far)\n",
    "    drawdown_array = pd.Series(drawdowns, index=pnl.index)\n",
    "    return drawdown_array.min(), max_drawdown_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk Attribution Analysis Framework\n",
    "\n",
    "def weights_update(weights,returns):\n",
    "    # check dimensions\n",
    "    if len(weights) != returns.shape[1]:\n",
    "        raise ValueError('Dimensions of Returns and Weights do not match')\n",
    "    \n",
    "    new_weights = np.zeros((returns.shape[0],len(weights)))\n",
    "    \n",
    "    for i in range(returns.shape[0]):\n",
    "        new_weights[i,:] = weights\n",
    "        weights *= (1+returns[i,:])\n",
    "        Rt = sum(weights)-1\n",
    "        weights = weights/(Rt+1)\n",
    "\n",
    "    return new_weights\n",
    "\n",
    "def carinok_calculate(port_returns):\n",
    "    totalreturn = (1+port_returns).prod(axis=0)-1\n",
    "    k = np.log(1+totalreturn)/totalreturn\n",
    "    \n",
    "    de_no = port_returns.copy()\n",
    "    de_no[de_no == 0] = 1\n",
    "    cari_fact = np.log(1+port_returns)/(de_no*k)\n",
    "\n",
    "    return cari_fact\n",
    "\n",
    "\"\"\"\n",
    "Total Return excluding portfolio\n",
    "\"\"\"\n",
    "def port_return_calculate(updated_factor_rets,resid_rets):\n",
    "    total_rets = np.hstack((updated_factor_rets.values, resid_rets[:, np.newaxis]))\n",
    "    total = np.exp(np.sum(np.log(total_rets+1),axis=0))-1\n",
    "    return total\n",
    "\n",
    "\"\"\"\n",
    "Return attribution\n",
    "\"\"\"\n",
    "\n",
    "def return_attribution_calculate(weighted_returns):\n",
    "    port_returns = weighted_returns.sum(axis=1)\n",
    "    carinok = carinok_calculate(port_returns)\n",
    "    return_attribution = carinok @ weighted_returns\n",
    "    return_attribution = np.append(return_attribution,return_attribution.sum())\n",
    "    return return_attribution\n",
    "\n",
    "\"\"\"\n",
    "Vol Attribution\n",
    "\"\"\"\n",
    "def risk_attribution_calculate(weighted_returns):\n",
    "    port_returns = weighted_returns.sum(axis=1)\n",
    "    risk = port_returns.std(ddof=1)\n",
    "\n",
    "    n = weighted_returns.shape[1]\n",
    "    risk_attribution = np.zeros(n+1)\n",
    "\n",
    "    for i in range(n):\n",
    "        model = sm.OLS(weighted_returns[:,i],sm.add_constant(port_returns))\n",
    "        results = model.fit()\n",
    "\n",
    "        risk_attribution[i] = results.params[1] * risk\n",
    "    risk_attribution[n] = risk\n",
    "    return risk_attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairs - To extract portfolio NAV from backtest results \n",
    "backtest = api.ReadBacktest(16513464, '12da9b60e487a26031f3ad3af3a6887e') \n",
    "chartpoint_ls = backtest.Charts[\"Strategy Equity\"].Series[\"Equity\"].Values \n",
    "nav_pair17 = [x.Close for x in chartpoint_ls if x.Time.hour != 5] \n",
    "date = [x.Time for x in chartpoint_ls if x.Time.hour != 5] \n",
    "nav_pair17 = pd.Series(nav_pair17,index=date)\n",
    "pair17 = nav_pair17.pct_change().dropna()\n",
    "\n",
    "backtest = api.ReadBacktest(16513464, '855fffa38804a69659f9fa0e98906b40') \n",
    "chartpoint_ls = backtest.Charts[\"Strategy Equity\"].Series[\"Equity\"].Values \n",
    "nav_pair22 = [x.Close for x in chartpoint_ls if x.Time.hour != 5] \n",
    "date = [x.Time for x in chartpoint_ls if x.Time.hour != 5] \n",
    "nav_pair22 = pd.Series(nav_pair22,index=date)\n",
    "pair22 = nav_pair22.pct_change().dropna()\n",
    "\n",
    "backtest = api.ReadBacktest(16513464, '5d793b15bf5d6e30c16712829470fa2d') \n",
    "chartpoint_ls = backtest.Charts[\"Strategy Equity\"].Series[\"Equity\"].Values \n",
    "nav_pair16 = [x.Close for x in chartpoint_ls if x.Time.hour != 5] \n",
    "date = [x.Time for x in chartpoint_ls if x.Time.hour != 5] \n",
    "nav_pair16 = pd.Series(nav_pair16,index=date)\n",
    "pair16 = nav_pair16.pct_change().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a2c\n",
    "backtest_e1 = api.ReadBacktest(16527795, '1956e15c56f7188cde0a6faf081cdd5b') \n",
    "chartpoint_ls = backtest_e1.Charts[\"Strategy Equity\"].Series[\"Equity\"].Values \n",
    "nav_a2c17 = [x.Close for x in chartpoint_ls if x.Time.hour != 5] \n",
    "date = [x.Time for x in chartpoint_ls if x.Time.hour != 5] \n",
    "nav_a2c17 = pd.Series(nav_a2c17,index=date)\n",
    "a2c17 = nav_a2c17.pct_change().dropna()\n",
    "\n",
    "backtest_e2 = api.ReadBacktest(16527795, '3b99b432a90e0def3b9f0593a2110b70')\n",
    "chartpoint_ls = backtest_e2.Charts[\"Strategy Equity\"].Series[\"Equity\"].Values \n",
    "nav_a2c16 = [x.Close for x in chartpoint_ls if x.Time.hour != 5] \n",
    "date = [x.Time for x in chartpoint_ls if x.Time.hour != 5] \n",
    "nav_a2c16 = pd.Series(nav_a2c16,index=date)\n",
    "a2c16 = nav_a2c16.pct_change().dropna()\n",
    "\n",
    "backtest_e3 = api.ReadBacktest(16527795, 'e665a5ac4e5b4a241dff4cfdd82cd4c5')\n",
    "chartpoint_ls = backtest_e3.Charts[\"Strategy Equity\"].Series[\"Equity\"].Values \n",
    "nav_a2c22 = [x.Close for x in chartpoint_ls if x.Time.hour != 5] \n",
    "date = [x.Time for x in chartpoint_ls if x.Time.hour != 5] \n",
    "nav_a2c22 = pd.Series(nav_a2c22,index=date)\n",
    "a2c22 = nav_a2c22.pct_change().dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTA\n",
    "backtest_q1 = api.ReadBacktest(16390435, '581ea4887e829b062d6c2794f80339d5') \n",
    "chartpoint_ls = backtest_q1.Charts[\"Strategy Equity\"].Series[\"Equity\"].Values \n",
    "nav_trend22 = [x.Close for x in chartpoint_ls if x.Time.hour != 5] \n",
    "date = [x.Time for x in chartpoint_ls if x.Time.hour != 5] \n",
    "nav_trend22 = pd.Series(nav_trend22,index=date)/10\n",
    "trend22 = nav_trend22.pct_change().dropna()\n",
    "\n",
    "backtest_q2 = api.ReadBacktest(16390435, '556a6229a3df0279155a081262cb4e91')\n",
    "chartpoint_ls = backtest_q2.Charts[\"Strategy Equity\"].Series[\"Equity\"].Values \n",
    "nav_trend17 = [x.Close for x in chartpoint_ls if x.Time.hour != 5] \n",
    "date = [x.Time for x in chartpoint_ls if x.Time.hour != 5] \n",
    "nav_trend17 = pd.Series(nav_trend17,index=date)/10\n",
    "trend17 = nav_trend17.pct_change().dropna()\n",
    "\n",
    "backtest_q3= api.ReadBacktest(16390435, '3f74422f4e57b37fd319bfac1cdc6f52')\n",
    "chartpoint_ls = backtest_q3.Charts[\"Strategy Equity\"].Series[\"Equity\"].Values \n",
    "nav_trend16 = [x.Close for x in chartpoint_ls if x.Time.hour != 5] \n",
    "date = [x.Time for x in chartpoint_ls if x.Time.hour != 5] \n",
    "nav_trend16 = pd.Series(nav_trend16,index=date)/10\n",
    "trend16 = nav_trend16.pct_change().dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend17 = trend17.resample('D').last().dropna()\n",
    "a2c17 = a2c17.resample('D').last().dropna()\n",
    "pair17 = pair17.resample('D').last().dropna()\n",
    "df_17_21 = pd.concat([trend17,a2c17,pair17],axis=1)\n",
    "df_17_21.columns = ['cs','rl','arb']\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = df_17_21.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(3, 4))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5},annot=True, fmt=\".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharpe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17-21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nav_trend17 = nav_trend17.resample('D').last().dropna()\n",
    "nav_a2c17 = nav_a2c17.resample('D').last().dropna()\n",
    "nav_pair17 = nav_pair17.resample('D').last().dropna()\n",
    "nav17 = (nav_trend17 + nav_a2c17 + nav_pair17)/3\n",
    "pd.concat([nav_trend17, nav_a2c17, nav_pair17, nav17],axis=1).plot()\n",
    "sharpe_trend17 = (nav_trend17.pct_change().mean()/nav_trend17.pct_change().std()) * np.sqrt(252)\n",
    "sharpe_a2c17 = (nav_a2c17.pct_change().mean()/nav_a2c17.pct_change().std()) * np.sqrt(252)\n",
    "sharpe_pair17 = (nav_pair17.pct_change().mean()/nav_pair17.pct_change().std()) * np.sqrt(252)\n",
    "sharpe_nav17 = (nav17.pct_change().mean()/nav17.pct_change().std()) * np.sqrt(252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpe_nav17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_max_drawdown(nav17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nav_trend22 = nav_trend22.resample('D').last().dropna()\n",
    "nav_a2c22 = nav_a2c22.resample('D').last().dropna()\n",
    "nav_pair22 = nav_pair22.resample('D').last().dropna()\n",
    "nav22 = (nav_trend22 + nav_a2c22 + nav_pair22)/3\n",
    "pd.concat([nav_trend22, nav_a2c22, nav_pair22, nav22],axis=1).plot()\n",
    "plt.show()\n",
    "sharpe_trend22 = (nav_trend22.pct_change().mean()/nav_trend22.pct_change().std()) * np.sqrt(252)\n",
    "sharpe_a2c22 = (nav_a2c22.pct_change().mean()/nav_a2c22.pct_change().std()) * np.sqrt(252)\n",
    "sharpe_pair22 = (nav_pair22.pct_change().mean()/nav_pair22.pct_change().std()) * np.sqrt(252)\n",
    "sharpe_nav22 = (nav22.pct_change().mean()/nav22.pct_change().std()) * np.sqrt(252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_nav = 0.5*nav_a2c22+0.5*nav_pair22\n",
    "(t_nav.pct_change().mean()/t_nav.pct_change().std()) * np.sqrt(252)\n",
    "\n",
    "pd.concat([nav_a2c22, nav_pair22, t_nav],axis=1).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpe_nav22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_max_drawdown(nav22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16-17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nav_trend16 = nav_trend16.resample('D').last().dropna()\n",
    "nav_a2c16 = nav_a2c16.resample('D').last().dropna()\n",
    "nav_pair16 = nav_pair16.resample('D').last().dropna()\n",
    "nav16 = (nav_trend16 + nav_a2c16 + nav_pair16)/3\n",
    "pd.concat([nav_trend16, nav_a2c16, nav_pair16, nav16],axis=1).plot()\n",
    "plt.show()\n",
    "sharpe_trend16 = (nav_trend16.pct_change().mean()/nav_trend16.pct_change().std()) * np.sqrt(252)\n",
    "sharpe_a2c16 = (nav_a2c16.pct_change().mean()/nav_a2c16.pct_change().std()) * np.sqrt(252)\n",
    "sharpe_pair16 = (nav_pair16.pct_change().mean()/nav_pair16.pct_change().std()) * np.sqrt(252)\n",
    "sharpe_nav16 = (nav16.pct_change().mean()/nav16.pct_change().std()) * np.sqrt(252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpe_nav16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_max_drawdown(nav16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Sample Risk Attribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "rtns = pd.concat([trend17, a2c17, pair17],axis=1)\n",
    "rtns.columns = ['trend','a2c','pair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "init_weights = np.array([1/3,1/3,1/3])\n",
    "updated_weights = weights_update(init_weights,rtns.values)\n",
    "weighted_rets = updated_weights * rtns.values\n",
    "returnattribution = return_attribution_calculate(weighted_rets)\n",
    "returnattribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "risk_atrribution = risk_attribution_calculate(weighted_rets)\n",
    "risk_atrribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk Parity Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pure Risk Parity Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cov_mat = rtns.cov()\n",
    "weights = risk_parity_portfolio_on_sse(cov_mat.values)\n",
    "print('Weights for Trend, a2c, pair', weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Parity on Expected Shortfall Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cov_mat = rtns.cov()\n",
    "weights = risk_parity_port_es(rtns)\n",
    "print('Weights for Trend, a2c, pair', weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply the pure return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22.1-22.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "rtns22 = pd.concat([trend22, a2c22, pair22],axis=1)\n",
    "rtns22.columns = ['trend','a2c','pair']\n",
    "\n",
    "nav22 = 0.30580517*nav_trend22 + 0.2744029*nav_a2c22 + 0.41979193*nav_pair22\n",
    "pd.concat([nav_trend22, nav_a2c22, nav_pair22, nav22],axis=1).plot()\n",
    "plt.show()\n",
    "\n",
    "sharpe_nav22 = (nav22.pct_change().mean()/nav22.pct_change().std()) * np.sqrt(252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sharpe_nav22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16-17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "rtns16 = pd.concat([trend16, a2c16, pair16],axis=1)\n",
    "rtns16.columns = ['trend','a2c','pair']\n",
    "\n",
    "nav16 =  0.30580517*nav_trend16 + 0.2744029*nav_a2c16 + 0.41979193*nav_pair16\n",
    "pd.concat([nav_trend16, nav_a2c16, nav_pair16, nav16],axis=1).plot()\n",
    "plt.show()\n",
    "\n",
    "sharpe_nav16 = (nav16.pct_change().mean()/nav16.pct_change().std()) * np.sqrt(252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sharpe_nav16"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
